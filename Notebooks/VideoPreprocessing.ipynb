{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2189e02b",
   "metadata": {},
   "source": [
    "## Video Preprocessing :- Clipping Videos into Frames\n",
    "\n",
    "### Goal\n",
    "\n",
    "Prepare the RWF-2000 videos for model training by converting each long video into multiple fixed-length clips, and saving each clip as a sequence of RGB frames (images). This yields uniform inputs, increases training samples, and simplifies I/O.\n",
    "\n",
    "### Dataset & Split\n",
    "\n",
    "Source: RWF-2000 (Real-World Violence)\n",
    "\n",
    "Labels: violent, non_violent\n",
    "\n",
    "Split (80/20): Train = 800 violent + 800 non_violent, Val = 200 violent + 200 non_violent\n",
    "\n",
    "Unit of training: one clip = 15 frames (fixed length)\n",
    "\n",
    "### Why “video → clips → frames”?\n",
    "\n",
    "Uniform length: Most video models expect a fixed temporal size (e.g., 15/16 frames).\n",
    "\n",
    "More data: One video → many clips → better coverage of the action.\n",
    "\n",
    "Fast & simple I/O: Reading pre-decoded images is often faster and more robust than on-the-fly video decoding.\n",
    "\n",
    "Flexible modeling: Same frames can be used for 2D CNNs (per-frame), 3D CNNs (spatiotemporal), or transformer models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162ab5d",
   "metadata": {},
   "source": [
    "### Directory layout\n",
    "\n",
    "\"\"\" \n",
    "clips/\n",
    "  train/\n",
    "    violent/\n",
    "      v1/\n",
    "        clip_000/\n",
    "          frame_000.jpg\n",
    "          ...\n",
    "          frame_014.jpg\n",
    "        clip_001/\n",
    "        clip_002/\n",
    "      v2/\n",
    "        ...\n",
    "    non_violent/\n",
    "      ...\n",
    "  val/\n",
    "    violent/...\n",
    "    non_violent/...\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb537d",
   "metadata": {},
   "source": [
    "### Clipping Code -RWF-2000 Dataset.\n",
    "\n",
    "src >> vid_clipping.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0acf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_vid_full.py\n",
    "# End-to-end: make robust 3D clips from RWF-2000 and train an R3D-18 classifier.\n",
    "\n",
    "import os, sys, glob, shutil, unicodedata, math, random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ---------- Safe print for Windows consoles ----------\n",
    "if hasattr(sys.stdout, \"reconfigure\"):\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\", errors=\"replace\")\n",
    "    sys.stderr.reconfigure(encoding=\"utf-8\", errors=\"replace\")\n",
    "try:\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\", errors=\"replace\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def safe_print(msg: str) -> None:\n",
    "    try:\n",
    "        print(msg, flush=True)\n",
    "    except Exception:\n",
    "        try:\n",
    "            print(msg.encode(\"ascii\", \"replace\").decode(\"ascii\"), flush=True)\n",
    "        except Exception:\n",
    "            print(\"[[unprintable message]]\", flush=True)\n",
    "\n",
    "# ---------- Config ----------\n",
    "DATASET_PATH   = r\"D:\\SEM2\\AML\\SafeVisionAIML\\Data\\Video\\rwf2000\"   # <- parent that has train/val\n",
    "CLIPS_OUT      = r\"D:\\SEM2\\AML\\SafeVisionAIML\\Data\\video\\rwf2000_clips\"            # <- where fixed-length clips will be saved\n",
    "\n",
    "# Clip-making params (OK defaults for 3D CNNs)\n",
    "CLIP_LEN       = 16          # frames per clip\n",
    "TARGET_FPS     = 6           # effective fps after sampling\n",
    "CLIP_OVERLAP   = 0.5         # 0.0..0.9\n",
    "IMG_SIZE       = 112         # 112 (C3D/R3D) or 224 (I3D)\n",
    "SAVE_AS_NPY    = False       # save JPEG frames (recommended); True -> save clip.npy per clip\n",
    "\n",
    "# Training params\n",
    "BATCH_SIZE     = 8           # reduce if OOM; CPU might need 2-4\n",
    "EPOCHS         = 15\n",
    "LR             = 1e-4\n",
    "WEIGHT_DECAY   = 1e-2\n",
    "SEED           = 42\n",
    "\n",
    "# Toggle steps\n",
    "REGENERATE_CLIPS = True      # set False if clips already prepared\n",
    "CLEAN_BROKEN     = True      # remove any clip_* dirs with wrong frame counts\n",
    "\n",
    "# ---------- Imports that need installed deps ----------\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def unicode_ascii_safe(name: str) -> str:\n",
    "    # Normalize to ASCII-safe name for directory\n",
    "    s = unicodedata.normalize(\"NFKD\", name).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return \"\".join(c if c.isalnum() else \"_\" for c in s)\n",
    "\n",
    "def detect_split_label(dataset_root: str, video_path: str) -> Tuple[str, str]:\n",
    "    rel = os.path.relpath(video_path, dataset_root).replace(\"\\\\\", \"/\").lower()\n",
    "    parts = rel.split(\"/\")\n",
    "\n",
    "    # --- split ---\n",
    "    split = \"unknown\"\n",
    "    for s in (\"train\", \"val\", \"valid\", \"validation\", \"test\"):\n",
    "        if s in parts:\n",
    "            split = \"val\" if s in (\"val\", \"valid\", \"validation\") else s\n",
    "            break\n",
    "\n",
    "    # --- label ---\n",
    "    # We keep internal labels as \"Fight\" / \"NonFight\" to match the rest of your code.\n",
    "    label = \"unknown\"\n",
    "    for p in parts:\n",
    "        if p in (\"violence\", \"violent\", \"fight\", \"fights\"):\n",
    "            label = \"Fight\"; break\n",
    "        if p in (\"nonviolence\", \"non-violence\", \"non_violence\", \"nonviolent\", \"normal\", \"nonfight\", \"non-fight\", \"non_fight\"):\n",
    "            label = \"NonFight\"; break\n",
    "\n",
    "    return split, label\n",
    "\n",
    "\n",
    "def read_frame(cap):\n",
    "    ok, fr = cap.read()\n",
    "    return fr if ok else None\n",
    "\n",
    "def resize_frame(frame, size: int):\n",
    "    return cv2.resize(frame, (size, size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def save_clip_jpegs(out_dir: str, frames: List[np.ndarray]) -> bool:\n",
    "    ensure_dir(out_dir)\n",
    "    ok_all = True\n",
    "    for i, fr in enumerate(frames):\n",
    "        fp = os.path.join(out_dir, f\"img_{i:03d}.jpg\")\n",
    "        ok = cv2.imwrite(fp, fr)\n",
    "        if not ok:\n",
    "            ok_all = False\n",
    "            break\n",
    "    if not ok_all:\n",
    "        shutil.rmtree(out_dir, ignore_errors=True)\n",
    "    return ok_all\n",
    "\n",
    "def save_clip_npy(out_dir: str, frames: List[np.ndarray]) -> bool:\n",
    "    ensure_dir(out_dir)\n",
    "    arr = np.stack(frames, axis=0)  # (T, H, W, C)\n",
    "    fp = os.path.join(out_dir, \"clip.npy\")\n",
    "    try:\n",
    "        np.save(fp, arr)\n",
    "        return True\n",
    "    except Exception:\n",
    "        shutil.rmtree(out_dir, ignore_errors=True)\n",
    "        return False\n",
    "\n",
    "# ---------- Clip maker ----------\n",
    "def make_clips_from_video(video_path: str) -> int:\n",
    "    split, label = detect_split_label(DATASET_PATH, video_path)\n",
    "    if split == \"unknown\" or label == \"unknown\":\n",
    "        safe_print(f\"[WARN] Unknown split/label -> skip: {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        safe_print(f\"[SKIP] Cannot open: {video_path}\")\n",
    "        return 0\n",
    "\n",
    "    src_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if not src_fps or src_fps <= 1e-3:\n",
    "        src_fps = 30.0  # fallback\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    if total_frames <= 0:\n",
    "        safe_print(f\"[SKIP] No frames: {video_path}\")\n",
    "        cap.release()\n",
    "        return 0\n",
    "\n",
    "    sample_step = max(int(round(src_fps / TARGET_FPS)), 1)\n",
    "    sampled_indices = list(range(0, total_frames, sample_step))\n",
    "    if len(sampled_indices) == 0:\n",
    "        safe_print(f\"[SKIP] No sampled frames: {video_path}\")\n",
    "        cap.release()\n",
    "        return 0\n",
    "\n",
    "    stride = max(int(round(CLIP_LEN * (1.0 - CLIP_OVERLAP))), 1)\n",
    "    starts = list(range(0, max(len(sampled_indices) - CLIP_LEN + 1, 1), stride))\n",
    "    if len(sampled_indices) < CLIP_LEN:\n",
    "        starts = [0]\n",
    "\n",
    "    vname = unicode_ascii_safe(os.path.splitext(os.path.basename(video_path))[0])\n",
    "    base_out = os.path.join(CLIPS_OUT, split, label, vname)\n",
    "    ensure_dir(base_out)\n",
    "\n",
    "    def read_by_index(idx: int):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        fr = read_frame(cap)\n",
    "        if fr is None:\n",
    "            return None\n",
    "        return resize_frame(fr, IMG_SIZE)\n",
    "\n",
    "    saved = 0\n",
    "    for k, st in enumerate(starts):\n",
    "        needed = sampled_indices[st: st + CLIP_LEN]\n",
    "        if len(needed) < CLIP_LEN:\n",
    "            if not needed:\n",
    "                continue\n",
    "            last = needed[-1]\n",
    "            needed = needed + [last] * (CLIP_LEN - len(needed))\n",
    "\n",
    "        frames = []\n",
    "        last_ok = None\n",
    "        for idx in needed:\n",
    "            fr = read_by_index(idx)\n",
    "            if fr is None:\n",
    "                fr = last_ok\n",
    "                if fr is None:\n",
    "                    frames = []\n",
    "                    break\n",
    "            frames.append(fr)\n",
    "            last_ok = fr\n",
    "\n",
    "        if len(frames) != CLIP_LEN:\n",
    "            continue\n",
    "\n",
    "        clip_dir = os.path.join(base_out, f\"clip_{k:03d}\")\n",
    "        ok = save_clip_npy(clip_dir, frames) if SAVE_AS_NPY else save_clip_jpegs(clip_dir, frames)\n",
    "        if ok:\n",
    "            saved += 1\n",
    "\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "def process_dataset_to_clips():\n",
    "    exts = (\".avi\", \".mp4\", \".mov\", \".mkv\")\n",
    "    total_videos, total_clips = 0, 0\n",
    "    for root, _, files in os.walk(DATASET_PATH):\n",
    "        vids = [f for f in files if f.lower().endswith(exts)]\n",
    "        if not vids: \n",
    "            continue\n",
    "        safe_print(f\"Folder: {os.path.basename(root)} | videos: {len(vids)}\")\n",
    "        for i, f in enumerate(vids, 1):\n",
    "            vp = os.path.join(root, f)\n",
    "            safe_print(f\"  [{i}/{len(vids)}] {vp}\")\n",
    "            try:\n",
    "                n = make_clips_from_video(vp)\n",
    "                total_videos += 1\n",
    "                total_clips  += n\n",
    "            except Exception as e:\n",
    "                safe_print(f\"  [ERROR] {vp}: {e}\")\n",
    "    safe_print(f\"Clip build done. Videos={total_videos}, Clips={total_clips}\")\n",
    "\n",
    "def clean_broken_clips():\n",
    "    removed = 0\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        sdir = os.path.join(CLIPS_OUT, split)\n",
    "        if not os.path.isdir(sdir):\n",
    "            continue\n",
    "        for label in (\"Fight\", \"NonFight\"):\n",
    "            ldir = os.path.join(sdir, label)\n",
    "            if not os.path.isdir(ldir):\n",
    "                continue\n",
    "            for vname in os.listdir(ldir):\n",
    "                vpath = os.path.join(ldir, vname)\n",
    "                if not os.path.isdir(vpath):\n",
    "                    continue\n",
    "                for cdir in glob.glob(os.path.join(vpath, \"clip_*\")):\n",
    "                    if SAVE_AS_NPY:\n",
    "                        ok = os.path.isfile(os.path.join(cdir, \"clip.npy\"))\n",
    "                    else:\n",
    "                        imgs = glob.glob(os.path.join(cdir, \"img_*.jpg\"))\n",
    "                        ok = (len(imgs) == CLIP_LEN)\n",
    "                    if not ok:\n",
    "                        shutil.rmtree(cdir, ignore_errors=True)\n",
    "                        removed += 1\n",
    "    safe_print(f\"Cleaned {removed} broken clip folders.\")\n",
    "\n",
    "def count_dataset_items():\n",
    "    counts = {}\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        total = 0\n",
    "        for label in (\"Fight\", \"NonFight\"):\n",
    "            pattern = os.path.join(CLIPS_OUT, split, label, \"*\", \"clip_*\")\n",
    "            total += len(glob.glob(pattern))\n",
    "        counts[split] = total\n",
    "    safe_print(f\"Clips count -> train={counts.get('train',0)}, val={counts.get('val',0)}, test={counts.get('test',0)}\")\n",
    "    return counts\n",
    "\n",
    "# ---------- Dataset & Training ----------\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, split: str, augment: bool=False):\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.items: List[Tuple[str, int]] = []\n",
    "        for label_name, label_int in [(\"Fight\", 0), (\"NonFight\", 1)]:\n",
    "            base = os.path.join(CLIPS_OUT, split, label_name)\n",
    "            if not os.path.isdir(base):\n",
    "                continue\n",
    "            for vname in os.listdir(base):\n",
    "                vpath = os.path.join(base, vname)\n",
    "                if not os.path.isdir(vpath):\n",
    "                    continue\n",
    "                for clip_dir in sorted(glob.glob(os.path.join(vpath, \"clip_*\"))):\n",
    "                    if SAVE_AS_NPY:\n",
    "                        if os.path.isfile(os.path.join(clip_dir, \"clip.npy\")):\n",
    "                            self.items.append((clip_dir, label_int))\n",
    "                    else:\n",
    "                        imgs = glob.glob(os.path.join(clip_dir, \"img_*.jpg\"))\n",
    "                        if len(imgs) == CLIP_LEN:\n",
    "                            self.items.append((clip_dir, label_int))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def _augment_img(self, img: Image.Image) -> Image.Image:\n",
    "        if self.augment and random.random() < 0.5:\n",
    "            img = TF.hflip(img)\n",
    "        return img\n",
    "\n",
    "    def _load_clip_jpegs(self, clip_dir: str) -> torch.Tensor:\n",
    "        files = sorted(glob.glob(os.path.join(clip_dir, \"img_*.jpg\")))\n",
    "        frames = []\n",
    "        for fp in files:\n",
    "            img = Image.open(fp).convert(\"RGB\")\n",
    "            img = self._augment_img(img)\n",
    "            img = TF.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "            t = TF.pil_to_tensor(img).float() / 255.0\n",
    "            t = TF.normalize(t, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "            frames.append(t)\n",
    "        video = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "        return video\n",
    "\n",
    "    def _load_clip_npy(self, clip_dir: str) -> torch.Tensor:\n",
    "        arr = np.load(os.path.join(clip_dir, \"clip.npy\"))  # (T,H,W,C)\n",
    "        frames = []\n",
    "        for i in range(arr.shape[0]):\n",
    "            img = Image.fromarray(arr[i].astype(np.uint8))\n",
    "            img = self._augment_img(img)\n",
    "            img = TF.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "            t = TF.pil_to_tensor(img).float() / 255.0\n",
    "            t = TF.normalize(t, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "            frames.append(t)\n",
    "        video = torch.stack(frames, dim=1)  # (C, T, H, W)\n",
    "        return video\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        clip_dir, label = self.items[idx]\n",
    "        if SAVE_AS_NPY:\n",
    "            video = self._load_clip_npy(clip_dir)\n",
    "        else:\n",
    "            video = self._load_clip_jpegs(clip_dir)\n",
    "        return video, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def make_loaders(batch_size=BATCH_SIZE):\n",
    "    # workers=0 for Windows debugging; increase to 2 later if stable\n",
    "    pin = torch.cuda.is_available()\n",
    "    train_ds = ClipDataset(\"train\", augment=True)\n",
    "    val_ds   = ClipDataset(\"val\", augment=False)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=pin)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=0, pin_memory=pin)\n",
    "    safe_print(f\"Dataset -> train clips: {len(train_ds)}, val clips: {len(val_ds)}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def make_model(num_classes=2, finetune=True):\n",
    "    weights = R3D_18_Weights.KINETICS400_V1\n",
    "    model = r3d_18(weights=weights)\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_feats, num_classes)\n",
    "    if finetune:\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    # Confusion matrix: rows=gt (0,1), cols=pred (0,1)\n",
    "    cm = np.zeros((2,2), dtype=int)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (preds == y).sum().item()\n",
    "        for gt, pr in zip(y.cpu().numpy(), preds.cpu().numpy()):\n",
    "            cm[gt, pr] += 1\n",
    "    acc = correct / max(total, 1)\n",
    "    safe_print(f\"Val Acc: {acc:.4f}\")\n",
    "    safe_print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    return acc, cm\n",
    "\n",
    "def train():\n",
    "    set_seed()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    safe_print(f\"Device: {device}\")\n",
    "\n",
    "    train_loader, val_loader = make_loaders(BATCH_SIZE)\n",
    "\n",
    "    model = make_model(num_classes=2, finetune=True).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_path = os.path.join(CLIPS_OUT, \"best_r3d18.pth\")\n",
    "    ensure_dir(CLIPS_OUT)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        seen = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item() * y.size(0)\n",
    "            seen += y.size(0)\n",
    "        train_loss = running / max(seen, 1)\n",
    "        safe_print(f\"Epoch {epoch}/{EPOCHS} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        acc, _ = evaluate(model, val_loader, device)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            safe_print(f\"  ✓ New best: {best_acc:.4f} -> saved {best_path}\")\n",
    "\n",
    "    safe_print(f\"Training done. Best val acc = {best_acc:.4f}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "if __name__ == \"__main__\":\n",
    "    safe_print(f\"Prep: CLIP_LEN={CLIP_LEN}, TARGET_FPS={TARGET_FPS}, OVERLAP={CLIP_OVERLAP}, SIZE={IMG_SIZE}, SAVE_AS_NPY={SAVE_AS_NPY}\")\n",
    "\n",
    "    if REGENERATE_CLIPS:\n",
    "        safe_print(\"Building clips from videos...\")\n",
    "        process_dataset_to_clips()\n",
    "\n",
    "    if CLEAN_BROKEN:\n",
    "        safe_print(\"Cleaning broken/empty clip folders...\")\n",
    "        clean_broken_clips()\n",
    "\n",
    "    count_dataset_items()\n",
    "\n",
    "    safe_print(\"Starting training...\")\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf30bb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf4068d",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "Goal: ensure each training sample (a 15-frame clip) is valid, consistent, and leak-free.\n",
    "\n",
    "Checks\n",
    "\n",
    "Frame count: every clip_* contains exactly 15 frames (no empty/short clips).\n",
    "\n",
    "Image health: all frames open without errors (corruption check).\n",
    "\n",
    "Split leakage: same video_id does not appear in both train/ and val/.\n",
    "\n",
    "Structure: required folders exist; each video has at least one clip_*.\n",
    "\n",
    "Resolution sanity: record common (width × height) to spot outliers.\n",
    "\n",
    "Fix policy\n",
    "\n",
    "Corrupt / wrong-length / empty clips → drop the entire clip_*.\n",
    "\n",
    "Train/val overlap by video_id → rename or remove in one split.\n",
    "\n",
    "Missing / broken structure → re-run clipping for that subset.\n",
    "\n",
    "Artifacts saved\n",
    "\n",
    "issues.csv — list of problems (type, path, details).\n",
    "\n",
    "resolution_stats.csv — counts of frame resolutions for a quick table/plot.\n",
    "\n",
    "Clean dataset = pass\n",
    "\n",
    "Zero leakage, zero corrupt/short clips, 15 readable frames per clip, sensible resolution distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f022be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ab6cf00",
   "metadata": {},
   "source": [
    "### Data Transforms\n",
    "\n",
    "Resize to 224 and normalize (ImageNet mean/std); show one grid of original vs. augmented (random crop/flip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Transforms & a tiny visualization ===\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "CLIPS_ROOT = Path(r\"Data/Video/rwf2000_clips\")\n",
    "FIG_DIR = Path(\"Notebooks/figures\"); FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "to_tensor = T.Compose([\n",
    "    T.Resize(224), T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "augment = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "# pick one clip and a frame\n",
    "example = next((CLIPS_ROOT/\"train\"/\"Fight\").glob(\"*/clip_*\"), None)\n",
    "frame = sorted(example.glob(\"img_*.jpg\"))[0]\n",
    "img = Image.open(frame).convert(\"RGB\")\n",
    "\n",
    "# show 1 original + 5 augmented views\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,6,1); plt.imshow(img); plt.axis(\"off\"); plt.title(\"orig\")\n",
    "for i in range(5):\n",
    "    plt.subplot(1,6,2+i); plt.imshow(augment(img)); plt.axis(\"off\")\n",
    "plt.suptitle(\"Transforms: RandomResizedCrop + Flip (224)\"); plt.tight_layout()\n",
    "out = FIG_DIR/\"transforms_grid.png\"; plt.savefig(out, dpi=150); plt.show()\n",
    "print(\"saved:\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
